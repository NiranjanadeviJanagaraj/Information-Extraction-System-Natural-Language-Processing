{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_18230368.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "sC-LZ20S_WUr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Information Extraction - Assignment\n",
        "This assignment is based on the Information Extraction lecture and the lab.\n"
      ]
    },
    {
      "metadata": {
        "id": "njMG9t-1P8ps",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Assignment2\n",
        "Niranjanadevi Janagaraj\n",
        "18230368"
      ]
    },
    {
      "metadata": {
        "id": "9xqCFJBv_WUt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Importing Library\n",
        "import nltk\n",
        "import re\n",
        "from statistics import mode\n",
        "nltk.download('all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D0rx7gngRTBu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import itertools\n",
        "import json\n",
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lmtzr = WordNetLemmatizer()\n",
        "from difflib import SequenceMatcher"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m0mo13vi_WUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Loading File\n",
        "inputfile='football_player.txt' \n",
        "buf=open(inputfile, encoding=\"UTF-8\")\n",
        "document_list=buf.read().split('\\n')\n",
        "print(document_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SD3hL_cMqpu-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l = []\n",
        "for i in document_list:\n",
        "   if len(i) != 0:\n",
        "       l.append(i)\n",
        "document_list = l\n",
        "print(document_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DCEJrJ-p_WU1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 1 (10 Marks)\n",
        "Write a function that takes each document and performs:\n",
        "1) sen segmentation 2) tokenization 3) part-of-speech tagging\n",
        "\n",
        "Please keep in mind that the expected output is a list within a list as shown below.\n"
      ]
    },
    {
      "metadata": {
        "id": "U3MCJIcR_WU2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ie_preprocess(document):\n",
        "  #code goes here\n",
        "  sen = sent_tokenize(document)\n",
        "  sen_pos = []\n",
        "  pos_sent=[]\n",
        "  for i in sen:\n",
        "    text = word_tokenize(i)\n",
        "    sen_pos.append(nltk.pos_tag(text))\n",
        "  return sen_pos\n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-E04CUNb_WU6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the following code to check your result for the first document (Ronaldo)."
      ]
    },
    {
      "metadata": {
        "id": "R30taRgf_WU6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "first_doc=document_list[0]\n",
        "pos_sent=ie_preprocess(first_doc)\n",
        "pos_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OMw70Wm8_WU-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Expected output\n",
        " [...[('He', 'PRP'),\n",
        "  ('is', 'VBZ'),\n",
        "  ('a', 'DT'),\n",
        "  ('forward', 'NN'),\n",
        "  ('and', 'CC'),\n",
        "  ('serves', 'NNS'),\n",
        "  ('as', 'IN'),\n",
        "  ('captain', 'NN'),\n",
        "  ('for', 'IN'),\n",
        "  ('Portugal', 'NNP'),\n",
        "  ('.', '.')], ...]"
      ]
    },
    {
      "metadata": {
        "id": "tYTwrZId_WU_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 2 (20 Marks)\n",
        "Write a function that will take the list of tokens with POS tags for each sen and returns the named entities (NE). \n",
        "\n",
        "Hint: Use binary=True while calling NE chunk function"
      ]
    },
    {
      "metadata": {
        "id": "oGK2jEqE9XGq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "entities=[]\n",
        "def named_entity_finding(pos_sent):\n",
        "  tree = nltk.ne_chunk(pos_sent,binary=True)\n",
        "  for subtree in tree.subtrees():\n",
        "    if subtree.label() == 'NE':\n",
        "      entity = \"\"\n",
        "      for leaf in subtree.leaves():\n",
        "        entity = entity + leaf[0] + \" \"\n",
        "      entities.append(entity.strip())\n",
        "  return entities"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lUl_r8YhDWIP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pos_sent=ie_preprocess(document_list[0])\n",
        "named_entity_finding(pos_sent[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fDolbFKE_WVD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Expected output ['Cristiano Ronaldo',\n",
        " 'Santos Aveiro',\n",
        " 'ComM',\n",
        " 'GOIH',\n",
        " 'Portuguese',\n",
        " 'Portuguese',\n",
        " 'Spanish',\n",
        " 'Real Madrid',\n",
        " 'Portugal']"
      ]
    },
    {
      "metadata": {
        "id": "XHMp7xtK_WVE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 3 (10 Marks)\n",
        "\n",
        "Now use the named_entity_finding() function to extract all NEs for each document.\n",
        "\n",
        "Hint: pos_sents holds the list of lists of tokens with POS tags"
      ]
    },
    {
      "metadata": {
        "id": "TwFlzQx4_WVF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def NE_flat_list_fn(pos_sent):\n",
        "  NE=[]\n",
        "  for i in pos_sent:\n",
        "    new_pos = ie_preprocess(i)\n",
        "    for pos in new_pos:\n",
        "      entity = named_entity_finding(pos)\n",
        "      NE.append(entity)\n",
        "      NE_flat_list = list(itertools.chain.from_iterable(NE))\n",
        "  return NE_flat_list\n",
        "\n",
        "new=NE_flat_list_fn(document_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l7-ma9lJ_WVJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 4 (40 Marks)\n",
        "\n",
        "Write functions to extract the name of the player, country of origin and date of birth as well as the following relations: team(s) of the player and position(s) of the player.\n",
        "\n",
        "Hint: Use the re.compile() function to create the extraction patterns\n",
        "\n",
        "Reference: https://docs.python.org/3/howto/regex.html"
      ]
    },
    {
      "metadata": {
        "id": "tbaFyiah_WVK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def name_of_the_player(doc):\n",
        "  sen = sent_tokenize(doc)[0]\n",
        "  name=re.compile(r'^(\\w.+)(?=\\s+.born)')\n",
        "  name_player=name.match(sen)[0].split(\",\")[0]\n",
        "  return name_player\n",
        "\n",
        "def country_of_origin(doc):\n",
        "  country=[]\n",
        "  origin=re.compile(r'((?:[\\S,]+\\s+){0,1})national team')\n",
        "  for i, sent in enumerate(sent_tokenize(doc)):\n",
        "    if i != 2:\n",
        "      team = origin.findall(sent)\n",
        "      if len(team) != 0:\n",
        "        country.append(team[0])\n",
        "  return country\n",
        "\n",
        "def date_of_birth(doc):\n",
        "  born=[]\n",
        "  sen = sent_tokenize(doc)[0]\n",
        "  Date_of_birth = re.compile(r'born\\b\\s*((?:\\S+\\s+){0,3})')\n",
        "  birth = Date_of_birth.findall(sen)[0]\n",
        "  birth = re.sub('\\W+',' ', birth )\n",
        "  return birth\n",
        "\n",
        "\n",
        "    \n",
        "def position_of_the_player(doc):\n",
        "  position = [\"forward\", \"captain\", \"attacking midfielder\", \"striker\", \"winger\", \"central midfielder\", \"defensive tackle\", \"defensive end\"]\n",
        "  position_of_player=[]\n",
        "  player=[]\n",
        "  sent = sent_tokenize(doc)\n",
        "  for i, sent in enumerate(sent):\n",
        "    for j in position:\n",
        "      position_of_player = re.compile(r'\\b({0})\\b'.format(j))\n",
        "      ans = bool(position_of_player.search(sent))\n",
        "      if ans == True:\n",
        "        player.append(j)\n",
        "  return list(set(player))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OrYGK3PirxX_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def team_of_the_player(doc):\n",
        "  sen = sent_tokenize(doc)\n",
        "  new = []\n",
        "  team_of_name = []\n",
        "  national = []\n",
        "  named=[]\n",
        "  \n",
        "  pos_sent = ie_preprocess(doc)\n",
        "  for i, sent in enumerate(sen):\n",
        "    if i == 2:\n",
        "      break\n",
        "      \n",
        "      \n",
        "    match = re.compile(r'((?:[\\S,]+\\s+){0,1})national team')       \n",
        "    team_player = re.compile(r'club\\s+((?:[\\S,]+\\s*){0,2})')       \n",
        "    team = match.findall(sent)\n",
        "    if len(team_player .findall(sent)) != 0:\n",
        "      for f in team_player .findall(sent):\n",
        "        national.append(f)\n",
        "        \n",
        "    tree = nltk.ne_chunk(pos_sent[i],binary=False)                 \n",
        "    for subtree in tree.subtrees():\n",
        "      if subtree.label() == 'ORGANIZATION':\n",
        "        entity = \"\"\n",
        "        for leaf in subtree.leaves():\n",
        "          entity = entity + leaf[0] + \" \"\n",
        "        named.append(entity.strip())\n",
        "    team_of_name.append(named)                                      \n",
        "    if len(team) != 0:\n",
        "      l = team[0]+\"national team\"\n",
        "      new.append(l) \n",
        "      \n",
        "  if len(list(set(new))) != 0:\n",
        "    national_team = list(set(new))[0]\n",
        "  else:\n",
        "     national_team = country_of_origin(doc) \n",
        "  team_of_name = list(itertools.chain.from_iterable(team_of_name))\n",
        "  \n",
        "  \n",
        "  new_club = []\n",
        "  for i in national:\n",
        "    new_club.extend(nltk.word_tokenize(i))                       \n",
        "  \n",
        "  for i, s in enumerate(national):\n",
        "    national[i] = national[i].rstrip()\n",
        "  if len(list(set(team_of_name).intersection(national))) != 0:\n",
        "    team_of_name = list(set(team_of_name).intersection(national))   \n",
        "  \n",
        "  if len(list(set(new))) == 0:\n",
        "    team_of_name.append(national_team[0]+ \" national team\")       \n",
        "  else:\n",
        "    team_of_name.append(national_team)\n",
        "  team_of_name = list(set(team_of_name).difference(nltk.word_tokenize(sen[0])[0:6]))\n",
        "\n",
        "  return team_of_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "muftyDxECzoJ",
        "colab_type": "code",
        "outputId": "52ad3e62-68d4-41dd-8706-7de6f9aed513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "#Printing and checking answers\n",
        "\n",
        "print(name_of_the_player(document_list[0]))\n",
        "print(country_of_origin(document_list[1]))\n",
        "print(date_of_birth(document_list[1]))\n",
        "print(team_of_the_player(document_list[7]))\n",
        "print(position_of_the_player(document_list[1]))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cristiano Ronaldo dos Santos Aveiro\n",
            "['Argentina ']\n",
            "24 June 1987 \n",
            "['Arsenal', 'German national team']\n",
            "['captain', 'forward']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K-CNrMM5_WVO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Execute the below command to check your fuction\n"
      ]
    },
    {
      "metadata": {
        "id": "7g3p6CcQ_WVS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Expected output '5 February 1992'"
      ]
    },
    {
      "metadata": {
        "id": "sCYflDpp_WVU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 5 (10 Marks)\n",
        "\n",
        "Write a function using the outputs from the previous functions to generate JSON-LD output as follows.\n",
        "\n",
        "Reference: https://json-ld.org/primer/latest/\n",
        "\n",
        "{ \"@id\": \"http://my-soccer-ontology.com/footballer/name_of_the_player\",\n",
        "\n",
        "    \"name\": \"\",\n",
        "    \"born\": \"\",\n",
        "    \"country\": \"\",\n",
        "    \"position\": [\n",
        "        { \"@id\": \"http://my-soccer-ontology.com/position\",\n",
        "            \"type\": \"\"\n",
        "        }\n",
        "     ]   \n",
        "     \"team\": [\n",
        "        { \"@id\": \"http://my-soccer-ontology.com/team\",\n",
        "            \"name\": \"\"\n",
        "        }   \n",
        "     ]\n",
        "}\n"
      ]
    },
    {
      "metadata": {
        "id": "z8HStdlu_WVU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_jsonld(arg):\n",
        "  soccerld = { \"@id\": \"http://my-soccer-ontology.com/footballer/\"+arg[0],\n",
        "\n",
        "           \"name\": arg[0],\n",
        "           \"born\": arg[1],\n",
        "           \"country\": arg[2],\n",
        "           \"position\": [\n",
        "               { \"@id\": \"http://my-soccer-ontology.com/position/\",\n",
        "                   \"type\": arg[3]\n",
        "               }\n",
        "            ],   \n",
        "            \"team\": [\n",
        "               { \"@id\": \"http://my-soccer-ontology.com/team/\",\n",
        "                   \"name\": arg[4]\n",
        "               }   \n",
        "            ]\n",
        "       }\n",
        "\n",
        "  return json.dumps(soccerld)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YHDy5bgO3a_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "eeb1f09d-df9d-4384-eaa4-34883053b710"
      },
      "cell_type": "code",
      "source": [
        "def json_function(doc):\n",
        "  arguments = [name_of_the_player(doc), date_of_birth(doc), country_of_origin(doc), position_of_the_player(doc), team_of_the_player(doc)]\n",
        "  return arguments\n",
        "\n",
        "arguments=json_function(document_list[0])\n",
        "generate_jsonld(arguments)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"@id\": \"http://my-soccer-ontology.com/footballer/Cristiano Ronaldo dos Santos Aveiro\", \"name\": \"Cristiano Ronaldo dos Santos Aveiro\", \"born\": \"5 February 1985 \", \"country\": [\"Portugal \"], \"position\": [{\"@id\": \"http://my-soccer-ontology.com/position/\", \"type\": [\"captain\", \"forward\"]}], \"team\": [{\"@id\": \"http://my-soccer-ontology.com/team/\", \"name\": [\"Real Madrid\", \"Portugal national team\"]}]}'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "i3VtWxBr_WVZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Task 6 (10 Marks)\n",
        "Identify one other relation (besides team and player) and write a function to extract this. Also extend the JSON-LD output accordingly."
      ]
    },
    {
      "metadata": {
        "id": "TR0GZrUB_WVa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}